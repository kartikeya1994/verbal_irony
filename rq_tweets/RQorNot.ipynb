{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhetorical Question or Not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and func defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import preprocessor as tweetproc\n",
    "import re\n",
    "\n",
    "# import torch\n",
    "# import torch.autograd as autograd\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "MODAL_VERBS =  set([\"would\", \"will\", \"shall\", \"should\", \"can\", \"could\", \"may\", \"might\", \"ought\", \"must\"])\n",
    "NEGATIONS = None\n",
    "\n",
    "def load_negations(filename=None):\n",
    "    if filename is None:\n",
    "        filename = '../aligned_antonym_detect/negations.txt'\n",
    "    global NEGATIONS\n",
    "    with open(filename, 'r') as f: \n",
    "        ants = f.readlines()\n",
    "        ants = [a.strip() for a in ants]\n",
    "        NEGATIONS = set(ants)\n",
    "\n",
    "def load_data(rq_file, not_rq_file):\n",
    "    \"\"\"\n",
    "    Files contain tweet text on each line\n",
    "    returns samples, labels ([0,1] is RQ, [1,0] is not RQ)\n",
    "    \"\"\"\n",
    "    with open(rq_file, 'r') as f:\n",
    "        rq = f.readlines()\n",
    "        rq = [l.strip() for l in rq]\n",
    "    with open(not_rq_file, 'r') as f:\n",
    "        not_rq = f.readlines()\n",
    "        not_rq = [l.strip() for l in not_rq]\n",
    "    print(\"RQ: {}\\nNot RQ: {}\".format(len(rq), len(not_rq)))\n",
    "    rq_labels = [[0, 1]]*len(rq)\n",
    "    not_rq_labels  = [[1, 0]]*len(not_rq)\n",
    "    x = rq + not_rq\n",
    "    y = rq_labels + not_rq_labels\n",
    "    print(\"Total samples: {}\".format(len(x)))\n",
    "    return x, y\n",
    "\n",
    "def parse_embeddings(filename, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    format: word 0.3 0.8 0.1 ... \\n\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "    print('Found {} lines in embedding file {}'.format(len(lines)-1, filename))\n",
    "    \n",
    "    embeddings = {}\n",
    "    for line in lines[1:]: # first line is info\n",
    "        toks = line.split(' ')\n",
    "        assert len(toks) == (embedding_dim+1), \"Embedding length invalid: {}\".format(len(toks))\n",
    "        vec = toks[1:]\n",
    "        vec = [float(v) for v in vec]\n",
    "        embeddings[toks[0]] = np.array(vec)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def visualize(x, y, n=10):\n",
    "    for i in range(0, len(x), len(x)/n):\n",
    "        print np.argmax(y[i]), x[i],'\\n'\n",
    "        \n",
    "def remove_taboo(x):\n",
    "    taboo_list = ['#sarcasm', '#Sarcasm', '#SARCASM', '#sarcastic', '&amp;']\n",
    "    for taboo in taboo_list:\n",
    "        if taboo in x:\n",
    "            x = x.replace(taboo, '')\n",
    "    return x\n",
    "\n",
    "def replace_identifiers(x):\n",
    "    replace_list = [('$MENTION$', 'TOUSER'),\n",
    "                    ('$URL$', 'URL'),\n",
    "                    ('$NUMBER$', 'NUMBER')]\n",
    "    for tok, sub in replace_list:\n",
    "        x = x.replace(tok, sub)\n",
    "    return x\n",
    "\n",
    "def contains_digit(w):\n",
    "    return bool(re.search(r'\\d', w))\n",
    "\n",
    "def contains_alpha(w):\n",
    "    return bool(re.search(r'[a-zA-Z]',w))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings, vocab, negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num modal verbs:  10\n",
      "Num negations:  74\n",
      "Found 105698 lines in embedding file ../tweet_embeddings.txt\n",
      "Vocab size: 105698\n",
      "sample embedding:  [-0.091999 -0.090327 -0.116107  0.122594 -0.179668 -0.270206 -0.223126\n",
      " -0.50453   0.062491  0.070141 -0.084368 -0.105765 -0.214917  0.19564\n",
      " -0.115645  0.163886  0.019436  0.300362  0.168865  0.030936  0.274668\n",
      "  0.049551  0.180642  0.354748 -0.203798  0.50992   0.162908  0.085134\n",
      "  0.100754 -0.27362   0.067393  0.126837 -0.227833  0.112674  0.061513\n",
      " -0.185428  0.303224 -0.329283  0.081322 -0.01414  -0.51638  -0.077393\n",
      "  0.054275  0.232466  0.022979 -0.235335  0.28811   0.538704  0.381486\n",
      " -0.027811 -0.006613  0.223376  0.227421  0.017996  0.093377 -0.13187\n",
      "  0.140716  0.061507  0.073246 -0.063014 -0.187818  0.124506  0.229867\n",
      " -0.099627 -0.008644  0.295628 -0.058905  0.081961  0.459971  0.340804\n",
      " -0.267998 -0.453933  0.009051  0.182248  0.070721  0.027988  0.028843\n",
      "  0.207732  0.242723 -0.111458 -0.156547  0.152425 -0.056641 -0.091101\n",
      "  0.067743  0.173993 -0.044511 -0.199395  0.259184  0.007573 -0.336524\n",
      "  0.035532 -0.321585 -0.280664 -0.501805  0.26466  -0.141617  0.260318\n",
      "  0.164392 -0.063217]\n",
      "embedding dim:  (100,)\n"
     ]
    }
   ],
   "source": [
    "load_negations()\n",
    "print 'Num modal verbs: ', len(MODAL_VERBS)\n",
    "print 'Num negations: ', len(NEGATIONS)\n",
    "\n",
    "emb = parse_embeddings('../tweet_embeddings.txt')\n",
    "vocab = list(emb.keys())\n",
    "vocab.sort()\n",
    "print('Vocab size: {}'.format(len(vocab)))\n",
    "sample_emb = emb['😊']\n",
    "print 'embedding dim: ', sample_emb.shape\n",
    "print 'sample embedding: ', sample_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tweets and do processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RQ: 3402\n",
      "Not RQ: 4081\n",
      "Total samples: 7483\n",
      "1 @cmigbear You’re not going to give Amazon access to your house?!?   \n",
      "\n",
      "1 @scottsauls \"Hunting\"?  What are we animals or something!? #Offended  \n",
      "\n",
      "1 @business More contentious than Melanoma's coat? Yes or no??? #snark  #fashionfauxpas \n",
      "\n",
      "1 @JanineZeeCheng But the weather was SOOOOO bad!!  Why can't that fine, clearly-concerned gentleman make sure his date stays warm?!? #mansplaining  \n",
      "\n",
      "1 I can still use this in my car right?  https://t.co/B0OOAl74Mb \n",
      "\n",
      "0 @Dr_Ehmad Govt. is at fault. They made a mistake. No? \n",
      "\n",
      "0 this guy walked up to me and was coming way to close  he shaked my hand and wouldn’t let it go?¿ he also asked for my number and I told him no but he didn’t accept it \n",
      "\n",
      "0 RT @jlist: Which hairstyle do you like? https://t.co/hZojrnfXpm \n",
      "\n",
      "0 #MAGA Dear @USAGSessions, why are you dragging your feet on Hillary, Obama,  DNC, while we spare no expense to find  dirt on Trump? \n",
      "\n",
      "0 RT @Brother_Kin: 🔴LIVE!  👉Mystery game stream is a go! Game 1: Now Playing Game 2: ??? Game 3: ???  🤘https://t.co/GizEIjZjSq  #TeamEmmmmsie… \n",
      "\n",
      "0 Do u think maybe Trump's soul belongs to golf, and he hates all of this and just wants to golf? Maybe he's really good at it and honing his craft. Poor guy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rq_file = 'rq.txt.dedup'\n",
    "not_rq_file = 'not_rq.txt.dedup'\n",
    "\n",
    "tweets,y = load_data(rq_file, not_rq_file)\n",
    "y_integer = [np.argmax(y_i) for y_i in y ] # store integer labels instead of one-hot vec\n",
    "y_integer = np.array(y_integer)\n",
    "\n",
    "tweets = [remove_taboo(t) for t in tweets]\n",
    "visualize(tweets, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_special(tweets):\n",
    "    \"\"\"\n",
    "    URL: p.OPT.URL\n",
    "    Mention: p.OPT.MENTION\n",
    "    Hashtag: p.OPT.HASHTAG\n",
    "    Reserved Words: p.OPT.RESERVED\n",
    "    Emoji: p.OPT.EMOJI\n",
    "    Smiley: p.OPT.SMILEY\n",
    "    Number: p.OPT.NUMBER\n",
    "    \"\"\"\n",
    "    tweetproc.set_options(tweetproc.OPT.URL, tweetproc.OPT.MENTION)\n",
    "    tweets = [replace_identifiers(tweetproc.tokenize(t)) for t in tweets]\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 TOUSER You’re not going to give Amazon access to your house?!? \n",
      "\n",
      "1 Starting to see TOUSER philosophy at work now. TOUSER with 14 9 at halftime. Not even sure why he's on the floor?! \n",
      "\n",
      "1 TOUSER Well Sean, if you look at BOTH sides of da science I'm sure there's a \"deal\" we can make with nature right? #climateChangeIsReal \n",
      "\n",
      "1 Oh really? Who would've guessed Glad to see research examining validating what many students of Color share everyday #RacismInEd URL \n",
      "\n",
      "1 RT TOUSER: Going out on a limb, but could it be because she is a black woman? #ImpeachTrump URL \n",
      "\n",
      "1 TOUSER Alright. Who’s paying you guys to say that now? \n",
      "\n",
      "1 UNC gets away with academic fraud, now Duke gets away with this. Is there no justice? #seriously #almostasbad URL \n",
      "\n",
      "1 Wait, we learned NOTHING from the TOUSER after game 1!!??? WTF, \n",
      "\n",
      "1 TOUSER TOUSER or just kidding? More like a lifetime ban on cabinet members flying commercial anywhere. \n",
      "\n",
      "1 We all know Misha's OTP is Trutin. Or is it Pump? TOUSER 😂😂😂 #Collusion #TrumpRussia URL \n",
      "\n",
      "1 Time to get the Dow 24K hat? Solid gold, perhaps? But Dow up another 100 points this morning. Thanks, $IBM! \n",
      "\n",
      "1 TOUSER What is 'World ' to say? This is absurd as Craig... Of course this is a case of 'white supremacy'... () \n",
      "\n",
      "1 RT TOUSER: TOUSER TOUSER We all have that problem. Right? 😂 \n",
      "\n",
      "1 TOUSER This is , right? #youvegottobekidding \n",
      "\n",
      "1 TOUSER TOUSER TOUSER You eat dogs?! How dare you! URL \n",
      "\n",
      "1 Do the Japanese have #irony and ? URL \n",
      "\n",
      "1 Hey guys, I wonder if TOUSER tested 100 pans to find the perfect pan for their new Pan Pizza? #StopPlayingThatAd #Tv #Radio #Youtube \n",
      "\n",
      "1 \"Why is your arm in a sling?” \"Oh nothing much. Took an Indigo yesterday, just an airline fracture” 😂😂😂 TOUSER \n",
      "\n",
      "1 RT TOUSER: TOUSER Couldn’t happen to a nicer brand-Javanka #whichmemberoftrump’sfamilygetsjailedfirst? TOUSER… \n",
      "\n",
      "1 Arent we supposed to go out and burn shit down? URL \n",
      "\n",
      "1 TOUSER This was about grass clippings leaves between the 2 lawns. People w/those beliefs are peaceful.....right? \n",
      "\n",
      "1 TOUSER But the inmates would know what the asylum needs most, right? #Snark \n",
      "\n",
      "1 So it's Obama's fault? URL \n",
      "\n",
      "1 RT TOUSER: TOUSER TOUSER Can you imagine Rand Paul not getting along with his neighbor? #notme \n",
      "\n",
      "1 TOUSER TOUSER TOUSER TOUSER What is yours? The loan ? 😂😂😝 \n",
      "\n",
      "1 Quick question... Is Ezekiel Elliot going to play Sunday? I just wanted to make sure. #IveSeenFortyTweetsAboutHim \n",
      "\n",
      "1 TOUSER Why? The editor can fix that in post \n",
      "\n",
      "1 TOUSER and what are you two other customs line are doing? Thats right, its bloody closed. Way to go giving a great impression of your capital city! URL \n",
      "\n",
      "1 I know, right? He's a regular Honest Abe... URL \n",
      "\n",
      "1 What just happened? I don't understand what just happened. #dontmindmeimjustpissy \n",
      "\n",
      "1 Who is this guy, just who does he think he is, some kind of comedian? If only everyone had the self control and self respect of our president. #irony #makedonalddrumpfagain URL \n",
      "\n",
      "1 TOUSER Why am I not surprised neither subject has a license? No paperwork for ownership or insurance. #Shock #LivePD \n",
      "\n",
      "1 TOUSER Reg, you must shush now! You know damn well that, as men, we’re not allowed to point out the bleeding obvious! After all, women are perfect and never do any wrong or harm to anyone, right? 😂 Man I love 🙂 \n",
      "\n",
      "1 TOUSER just want to ask... ? URL \n",
      "\n",
      "1 Why would a country like Canada pollute the majestic Fort Mac country side when the human rights abusers can enslave a few more people and turn the dial up a notch? #tarsands #FirstWorldOil URL \n",
      "\n",
      "1 #Rumi #Manto met TOUSER #TataLitFest What happened thereafter? Come watch this #soloplay TOUSER Fri 17 Nov dsts: bookmyshow / venue #theatre #mohitizm #ambalprod #india #pakistan #sufism #funny TOUSER TOUSER TOUSER URL8 \n",
      "\n",
      "1 TOUSER Happy Birthday. This douche gave you presents. Why are you not happy? \n",
      "\n",
      "1 TOUSER TOUSER TOUSER TOUSER TOUSER TOUSER TOUSER TOUSER Perhaps there was a bit of in the original tweet? You're making an even bigger fool of yourself beating that dead horse. \n",
      "\n",
      "1 First it was a cold, then a chesty cold, then pleurisy and now a sinus infection, what’s next?! I’ve literally been on and off amoxicillin for most of November and October. Tomorrow I’m on doxocycline for my sinus infection. Great. #beingillsucks #sinusitis \n",
      "\n",
      "1 TOUSER When did the name Pocahontas become a racial term?? It was one of My Daughters favorite movie, now I find it's a racist slur!!! Damm we will burn the DVD immediately. #unhingedleftistreporter \n",
      "\n",
      "1 TOUSER What could possibly go wrong?! \n",
      "\n",
      "1 RT TOUSER: Say it ain’t so!?! I can’t believe an ELITE would kill people for selfish gain. URL \n",
      "\n",
      "1 RT TOUSER: TOUSER Sexual predators in CA state government? Who would have guessed? . \n",
      "\n",
      "1 TOUSER #bergevin needs another goalie??? \n",
      "\n",
      "1 TOUSER TOUSER How do they do it?!? \n",
      "\n",
      "1 TOUSER I’m very disappointed with your attitude towards the housebound elderly. 81 y.o disabled mother-in-law told “the drs don’t come out for earache” - really? #perfect \n",
      "\n",
      "0 Why does my iPhone dial screen look like shit? URL \n",
      "\n",
      "0 WHY WOULD YOU ELECT TO DEFER WHEN YOUR OFFENSE HAS BEEN THE PACE SETTERS ALL YEAR? \n",
      "\n",
      "0 RT TOUSER: Is $40 WTI Now More Realistic Than $60? URL URLU \n",
      "\n",
      "0 Should she do a small gif spam? \n",
      "\n",
      "0 RT TOUSER: Keep Christ in Christmas? Can we put him back in Christianity first? \n",
      "\n",
      "0 Happy Black Friday! Had enough shopping and turkey to hold you over for a while? Don't worry,… URL8 \n",
      "\n",
      "0 How can you post a video with music? I keep getting pied 😂😂 \n",
      "\n",
      "0 TOUSER why must you be ridiculously photogenic? \n",
      "\n",
      "0 TOUSER Sorry to hear about the trouble! Do any error messages pop up when the app crashes? For now, try: URL1. \n",
      "\n",
      "0 RT TOUSER: Trump is passing out potato chips and turkey sandwiches. This is the best they can do for Thanksgiving? URL \n",
      "\n",
      "0 RT TOUSER: THIS IS HOW A DATE SHOULD BE PLANNED OUT instead of : “what do you wanna do?”. Ask your girl on a date with plans already m… \n",
      "\n",
      "0 What are the best herbs for stinky 'pits? \n",
      "\n",
      "0 RT TOUSER: Is that Martin McGuinness doing his assistant on the Sevco bench ??? 🙈 URL8 \n",
      "\n",
      "0 TOUSER TOUSER TOUSER TOUSER You are a bot, right? At least you sound like one. Improve the technology! \n",
      "\n",
      "0 TOUSER Dang He Hurt You That Bad ? 😏 \n",
      "\n",
      "0 TOUSER Why do people have such a Big problem with supporting each other? How is a rt going to kill you? Smh \n",
      "\n",
      "0 Here we have the unofficial, official defender of the little Hitlers defending their right to silence hundreds of thousands indiscriminately. Do you buy this “reasoning”? No, I didn’t think any thinking same person would. U picked the wrong woman to try your brand of reasoning on URL \n",
      "\n",
      "0 RT TOUSER: Thanksgiving? Well mind stuffing this Imp up? ~ URL \n",
      "\n",
      "0 RT TOUSER: knock knock who's there? not pinof 9 apparently \n",
      "\n",
      "0 RT TOUSER: wait why do people think the UK dates are going to be the only Europe dates?? she wouldn't only do one part of Europe th… \n",
      "\n",
      "0 RT TOUSER: 171124 nct127 ig live #JAEHYUN JH: \"We're practicing together now.. with all 127 members.. what is it for~? it's somethi… \n",
      "\n",
      "0 TOUSER After Miam loses to TOUSER and TOUSER beats TOUSER how about a rematch with TOUSER? \n",
      "\n",
      "0 TOUSER Am I ready for this type of greatness? URL \n",
      "\n",
      "0 MONKEY BUSINESS | remember this guy? I’ve been very busy working on sole cool projects but I took a break to do this little guy. Hopefully next year it will be finally a book. . . . . #luisgadea #gadea #monkey #animals #characterdesign #illustration #traditionalart #artistso… URL \n",
      "\n",
      "0 RT TOUSER: \"DID YOU THANK THE DOCTOR FOR BRINGING YOU INTO THIS WORLD?\" Lavar Ball is PURE comedy! 😂💀 URL \n",
      "\n",
      "0 TOUSER TOUSER Um... Let's go Bulls? \n",
      "\n",
      "0 I finally learned that if I read enough I won't have to worry about accidentally plagiarizing anymore. Plausible deniability and all. Have any other writers worried about this? URLD \n",
      "\n",
      "0 TOUSER I’m wrong or right? \n",
      "\n",
      "0 I told them not to screw it up...What do they do?...Screw it up smh. URL \n",
      "\n",
      "0 TOUSER And lose money as usual? \n",
      "\n",
      "0 Have you forgotten the 3 second rule? This video from TOUSER shows you how to better avoid accidents. URL \n",
      "\n",
      "0 #ThisDayInHistory 1947: House of Representatives cites #Hollywood Ten for contempt of #Congress. #Commies or Not Commies? Read more here URL #ushistory #ColdWar URL10 \n",
      "\n",
      "0 #women are under the most brutal suppression in #Iran,I'm surprised that you're so eager 2go there! Is this mission on behalf of TOUSER?Or is your delegation working on trade agreements W/ Iran? TOUSER TOUSER TOUSER TOUSER TOUSER TOUSER URL \n",
      "\n",
      "0 RT TOUSER: TOUSER TOUSER So their illegal and want to bring attention to themselves? 🤔 \n",
      "\n",
      "0 Wait, what? Someone explain what this means. URL \n",
      "\n",
      "0 TOUSER what is this? URL \n",
      "\n",
      "0 Are there any wireless controllers out there for the SNES Classic available for Canada? TOUSER TOUSER TOUSER TOUSER TOUSER #PleaseHelp #GameON \n",
      "\n",
      "0 RT TOUSER: What do you think of Christ? May we reflect on t/greatest blessing of all, t/true nature of Christ our Lord! Jesus, no o… \n",
      "\n",
      "0 TOUSER Is there another view of this? It does look like it could get swiped at as he’s going up. \n",
      "\n",
      "0 Found a Transponder Snail! Oars comes back as a henchman of Moria?! URL #TreCru URL \n",
      "\n",
      "0 TOUSER 🤨 is it bc you want it yourself? or ... another reason? \n",
      "\n",
      "0 TOUSER im early are you proud? URL \n",
      "\n",
      "0 Anybody wanna chill in a party or play pro-am? \n",
      "\n",
      "0 May be a coincidence, but didn't Egypt JUST open its border with Gaza strip recently after a long shutdown? Hmm. \"Militants Kill 235 in Attack on Sufi Mosque in Egypt\" NYT URL \n",
      "\n",
      "0 TOUSER TOUSER Will Volume 4 ever be reprinted? \n",
      "\n",
      "0 What needs to happen to get Central Florida into the playoff? Because that's what I'm rooting for. \n",
      "\n",
      "0 RT TOUSER: BAEK new \"fearless\" tattoo?? 👀 #EXO #TheElyXiOninSeoulday1 TOUSER URL2C \n",
      "\n",
      "0 TOUSER Lmaoooooooo whet?! You so out of order 😭😭 \n",
      "\n",
      "0 RT TOUSER: Did u know MI has the U.S. 1st state-focused #connectedtech trade assn? Let us be ur resource! URL #mobili… \n",
      "\n",
      "0 TOUSER TOUSER Next time you're coming TOUSER right? \n",
      "\n",
      "0 RT TOUSER: #PINOF9 Do you want Pinof 9 to be released now? -vote and rt pls- \n",
      "\n",
      "0 TOUSER This is the only acceptable outcome??? Instrument??? His hot bod \n",
      "\n",
      "0 Some people are so hard to understand.? \n",
      "\n",
      "0 RT TOUSER: Catfishing ah now could ye be bothered? \n",
      "\n",
      "0 // I LOVE how whenever my Snart says something like ‘Anyone wants to play?’ and another Snart comes to respond lmao it’s like ‘Ah, I got you, my lonely other self.’ It’s cute. Lol. \n",
      "\n",
      "0 TOUSER TOUSER When can we expect an update? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = replace_special(tweets)\n",
    "visualize(tweets, y, n = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def tokenize_tweets(tweets):\n",
    "    tweet_tokens = [nltk.word_tokenize(t.decode('utf-8')) for t in tweets]\n",
    "    return tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [u'TOUSER', u'You', u'\\u2019', u're', u'not', u'going', u'to', u'give', u'Amazon', u'access', u'to', u'your', u'house', u'?', u'!', u'?'] \n",
      "\n",
      "1 [u'Starting', u'to', u'see', u'TOUSER', u'philosophy', u'at', u'work', u'now', u'.', u'TOUSER', u'with', u'14', u'9', u'at', u'halftime', u'.', u'Not', u'even', u'sure', u'why', u'he', u\"'s\", u'on', u'the', u'floor', u'?', u'!'] \n",
      "\n",
      "1 [u'TOUSER', u'Well', u'Sean', u',', u'if', u'you', u'look', u'at', u'BOTH', u'sides', u'of', u'da', u'science', u'I', u\"'m\", u'sure', u'there', u\"'s\", u'a', u'``', u'deal', u\"''\", u'we', u'can', u'make', u'with', u'nature', u'right', u'?', u'#', u'climateChangeIsReal'] \n",
      "\n",
      "1 [u'Oh', u'really', u'?', u'Who', u'would', u\"'ve\", u'guessed', u'Glad', u'to', u'see', u'research', u'examining', u'validating', u'what', u'many', u'students', u'of', u'Color', u'share', u'everyday', u'#', u'RacismInEd', u'URL'] \n",
      "\n",
      "1 [u'RT', u'TOUSER', u':', u'Going', u'out', u'on', u'a', u'limb', u',', u'but', u'could', u'it', u'be', u'because', u'she', u'is', u'a', u'black', u'woman', u'?', u'#', u'ImpeachTrump', u'URL'] \n",
      "\n",
      "1 [u'TOUSER', u'Alright', u'.', u'Who', u'\\u2019', u's', u'paying', u'you', u'guys', u'to', u'say', u'that', u'now', u'?'] \n",
      "\n",
      "1 [u'UNC', u'gets', u'away', u'with', u'academic', u'fraud', u',', u'now', u'Duke', u'gets', u'away', u'with', u'this', u'.', u'Is', u'there', u'no', u'justice', u'?', u'#', u'seriously', u'#', u'almostasbad', u'URL'] \n",
      "\n",
      "1 [u'Wait', u',', u'we', u'learned', u'NOTHING', u'from', u'the', u'TOUSER', u'after', u'game', u'1', u'!', u'!', u'?', u'?', u'?', u'WTF', u','] \n",
      "\n",
      "1 [u'TOUSER', u'TOUSER', u'or', u'just', u'kidding', u'?', u'More', u'like', u'a', u'lifetime', u'ban', u'on', u'cabinet', u'members', u'flying', u'commercial', u'anywhere', u'.'] \n",
      "\n",
      "1 [u'We', u'all', u'know', u'Misha', u\"'s\", u'OTP', u'is', u'Trutin', u'.', u'Or', u'is', u'it', u'Pump', u'?', u'TOUSER', u'\\U0001f602\\U0001f602\\U0001f602', u'#', u'Collusion', u'#', u'TrumpRussia', u'URL'] \n",
      "\n",
      "1 [u'Time', u'to', u'get', u'the', u'Dow', u'24K', u'hat', u'?', u'Solid', u'gold', u',', u'perhaps', u'?', u'But', u'Dow', u'up', u'another', u'100', u'points', u'this', u'morning', u'.', u'Thanks', u',', u'$', u'IBM', u'!'] \n",
      "\n",
      "1 [u'TOUSER', u'What', u'is', u\"'World\", u\"'\", u'to', u'say', u'?', u'This', u'is', u'absurd', u'as', u'Craig', u'...', u'Of', u'course', u'this', u'is', u'a', u'case', u'of', u\"'white\", u'supremacy', u\"'\", u'...', u'(', u')'] \n",
      "\n",
      "1 [u'RT', u'TOUSER', u':', u'TOUSER', u'TOUSER', u'We', u'all', u'have', u'that', u'problem', u'.', u'Right', u'?', u'\\U0001f602'] \n",
      "\n",
      "1 [u'TOUSER', u'This', u'is', u',', u'right', u'?', u'#', u'youvegottobekidding'] \n",
      "\n",
      "1 [u'TOUSER', u'TOUSER', u'TOUSER', u'You', u'eat', u'dogs', u'?', u'!', u'How', u'dare', u'you', u'!', u'URL'] \n",
      "\n",
      "1 [u'Do', u'the', u'Japanese', u'have', u'#', u'irony', u'and', u'?', u'URL'] \n",
      "\n",
      "1 [u'Hey', u'guys', u',', u'I', u'wonder', u'if', u'TOUSER', u'tested', u'100', u'pans', u'to', u'find', u'the', u'perfect', u'pan', u'for', u'their', u'new', u'Pan', u'Pizza', u'?', u'#', u'StopPlayingThatAd', u'#', u'Tv', u'#', u'Radio', u'#', u'Youtube'] \n",
      "\n",
      "1 [u'``', u'Why', u'is', u'your', u'arm', u'in', u'a', u'sling', u'?', u'\\u201d', u'``', u'Oh', u'nothing', u'much', u'.', u'Took', u'an', u'Indigo', u'yesterday', u',', u'just', u'an', u'airline', u'fracture', u'\\u201d', u'\\U0001f602\\U0001f602\\U0001f602', u'TOUSER'] \n",
      "\n",
      "1 [u'RT', u'TOUSER', u':', u'TOUSER', u'Couldn', u'\\u2019', u't', u'happen', u'to', u'a', u'nicer', u'brand-Javanka', u'#', u'whichmemberoftrump', u'\\u2019', u'sfamilygetsjailedfirst', u'?', u'TOUSER\\u2026'] \n",
      "\n",
      "1 [u'Arent', u'we', u'supposed', u'to', u'go', u'out', u'and', u'burn', u'shit', u'down', u'?', u'URL'] \n",
      "\n",
      "1 [u'TOUSER', u'This', u'was', u'about', u'grass', u'clippings', u'leaves', u'between', u'the', u'2', u'lawns', u'.', u'People', u'w/those', u'beliefs', u'are', u'peaceful', u'...', u'..right', u'?'] \n",
      "\n",
      "1 [u'TOUSER', u'But', u'the', u'inmates', u'would', u'know', u'what', u'the', u'asylum', u'needs', u'most', u',', u'right', u'?', u'#', u'Snark'] \n",
      "\n",
      "1 [u'So', u'it', u\"'s\", u'Obama', u\"'s\", u'fault', u'?', u'URL'] \n",
      "\n",
      "1 [u'RT', u'TOUSER', u':', u'TOUSER', u'TOUSER', u'Can', u'you', u'imagine', u'Rand', u'Paul', u'not', u'getting', u'along', u'with', u'his', u'neighbor', u'?', u'#', u'notme'] \n",
      "\n",
      "1 [u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'What', u'is', u'yours', u'?', u'The', u'loan', u'?', u'\\U0001f602\\U0001f602\\U0001f61d'] \n",
      "\n",
      "1 [u'Quick', u'question', u'...', u'Is', u'Ezekiel', u'Elliot', u'going', u'to', u'play', u'Sunday', u'?', u'I', u'just', u'wanted', u'to', u'make', u'sure', u'.', u'#', u'IveSeenFortyTweetsAboutHim'] \n",
      "\n",
      "1 [u'TOUSER', u'Why', u'?', u'The', u'editor', u'can', u'fix', u'that', u'in', u'post'] \n",
      "\n",
      "1 [u'TOUSER', u'and', u'what', u'are', u'you', u'two', u'other', u'customs', u'line', u'are', u'doing', u'?', u'Thats', u'right', u',', u'its', u'bloody', u'closed', u'.', u'Way', u'to', u'go', u'giving', u'a', u'great', u'impression', u'of', u'your', u'capital', u'city', u'!', u'URL'] \n",
      "\n",
      "1 [u'I', u'know', u',', u'right', u'?', u'He', u\"'s\", u'a', u'regular', u'Honest', u'Abe', u'...', u'URL'] \n",
      "\n",
      "1 [u'What', u'just', u'happened', u'?', u'I', u'do', u\"n't\", u'understand', u'what', u'just', u'happened', u'.', u'#', u'dontmindmeimjustpissy'] \n",
      "\n",
      "1 [u'Who', u'is', u'this', u'guy', u',', u'just', u'who', u'does', u'he', u'think', u'he', u'is', u',', u'some', u'kind', u'of', u'comedian', u'?', u'If', u'only', u'everyone', u'had', u'the', u'self', u'control', u'and', u'self', u'respect', u'of', u'our', u'president', u'.', u'#', u'irony', u'#', u'makedonalddrumpfagain', u'URL'] \n",
      "\n",
      "1 [u'TOUSER', u'Why', u'am', u'I', u'not', u'surprised', u'neither', u'subject', u'has', u'a', u'license', u'?', u'No', u'paperwork', u'for', u'ownership', u'or', u'insurance', u'.', u'#', u'Shock', u'#', u'LivePD'] \n",
      "\n",
      "1 [u'TOUSER', u'Reg', u',', u'you', u'must', u'shush', u'now', u'!', u'You', u'know', u'damn', u'well', u'that', u',', u'as', u'men', u',', u'we', u'\\u2019', u're', u'not', u'allowed', u'to', u'point', u'out', u'the', u'bleeding', u'obvious', u'!', u'After', u'all', u',', u'women', u'are', u'perfect', u'and', u'never', u'do', u'any', u'wrong', u'or', u'harm', u'to', u'anyone', u',', u'right', u'?', u'\\U0001f602', u'Man', u'I', u'love', u'\\U0001f642'] \n",
      "\n",
      "1 [u'TOUSER', u'just', u'want', u'to', u'ask', u'...', u'?', u'URL'] \n",
      "\n",
      "1 [u'Why', u'would', u'a', u'country', u'like', u'Canada', u'pollute', u'the', u'majestic', u'Fort', u'Mac', u'country', u'side', u'when', u'the', u'human', u'rights', u'abusers', u'can', u'enslave', u'a', u'few', u'more', u'people', u'and', u'turn', u'the', u'dial', u'up', u'a', u'notch', u'?', u'#', u'tarsands', u'#', u'FirstWorldOil', u'URL'] \n",
      "\n",
      "1 [u'#', u'Rumi', u'#', u'Manto', u'met', u'TOUSER', u'#', u'TataLitFest', u'What', u'happened', u'thereafter', u'?', u'Come', u'watch', u'this', u'#', u'soloplay', u'TOUSER', u'Fri', u'17', u'Nov', u'dsts', u':', u'bookmyshow', u'/', u'venue', u'#', u'theatre', u'#', u'mohitizm', u'#', u'ambalprod', u'#', u'india', u'#', u'pakistan', u'#', u'sufism', u'#', u'funny', u'TOUSER', u'TOUSER', u'TOUSER', u'URL8'] \n",
      "\n",
      "1 [u'TOUSER', u'Happy', u'Birthday', u'.', u'This', u'douche', u'gave', u'you', u'presents', u'.', u'Why', u'are', u'you', u'not', u'happy', u'?'] \n",
      "\n",
      "1 [u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'Perhaps', u'there', u'was', u'a', u'bit', u'of', u'in', u'the', u'original', u'tweet', u'?', u'You', u\"'re\", u'making', u'an', u'even', u'bigger', u'fool', u'of', u'yourself', u'beating', u'that', u'dead', u'horse', u'.'] \n",
      "\n",
      "1 [u'First', u'it', u'was', u'a', u'cold', u',', u'then', u'a', u'chesty', u'cold', u',', u'then', u'pleurisy', u'and', u'now', u'a', u'sinus', u'infection', u',', u'what', u'\\u2019', u's', u'next', u'?', u'!', u'I', u'\\u2019', u've', u'literally', u'been', u'on', u'and', u'off', u'amoxicillin', u'for', u'most', u'of', u'November', u'and', u'October', u'.', u'Tomorrow', u'I', u'\\u2019', u'm', u'on', u'doxocycline', u'for', u'my', u'sinus', u'infection', u'.', u'Great', u'.', u'#', u'beingillsucks', u'#', u'sinusitis'] \n",
      "\n",
      "1 [u'TOUSER', u'When', u'did', u'the', u'name', u'Pocahontas', u'become', u'a', u'racial', u'term', u'?', u'?', u'It', u'was', u'one', u'of', u'My', u'Daughters', u'favorite', u'movie', u',', u'now', u'I', u'find', u'it', u\"'s\", u'a', u'racist', u'slur', u'!', u'!', u'!', u'Damm', u'we', u'will', u'burn', u'the', u'DVD', u'immediately', u'.', u'#', u'unhingedleftistreporter'] \n",
      "\n",
      "1 [u'TOUSER', u'What', u'could', u'possibly', u'go', u'wrong', u'?', u'!'] \n",
      "\n",
      "1 [u'RT', u'TOUSER', u':', u'Say', u'it', u'ain', u'\\u2019', u't', u'so', u'!', u'?', u'!', u'I', u'can', u'\\u2019', u't', u'believe', u'an', u'ELITE', u'would', u'kill', u'people', u'for', u'selfish', u'gain', u'.', u'URL'] \n",
      "\n",
      "1 [u'RT', u'TOUSER', u':', u'TOUSER', u'Sexual', u'predators', u'in', u'CA', u'state', u'government', u'?', u'Who', u'would', u'have', u'guessed', u'?', u'.'] \n",
      "\n",
      "1 [u'TOUSER', u'#', u'bergevin', u'needs', u'another', u'goalie', u'?', u'?', u'?'] \n",
      "\n",
      "1 [u'TOUSER', u'TOUSER', u'How', u'do', u'they', u'do', u'it', u'?', u'!', u'?'] \n",
      "\n",
      "1 [u'TOUSER', u'I', u'\\u2019', u'm', u'very', u'disappointed', u'with', u'your', u'attitude', u'towards', u'the', u'housebound', u'elderly', u'.', u'81', u'y.o', u'disabled', u'mother-in-law', u'told', u'\\u201c', u'the', u'drs', u'don', u'\\u2019', u't', u'come', u'out', u'for', u'earache', u'\\u201d', u'-', u'really', u'?', u'#', u'perfect'] \n",
      "\n",
      "0 [u'Why', u'does', u'my', u'iPhone', u'dial', u'screen', u'look', u'like', u'shit', u'?', u'URL'] \n",
      "\n",
      "0 [u'WHY', u'WOULD', u'YOU', u'ELECT', u'TO', u'DEFER', u'WHEN', u'YOUR', u'OFFENSE', u'HAS', u'BEEN', u'THE', u'PACE', u'SETTERS', u'ALL', u'YEAR', u'?'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'Is', u'$', u'40', u'WTI', u'Now', u'More', u'Realistic', u'Than', u'$', u'60', u'?', u'URL', u'URLU'] \n",
      "\n",
      "0 [u'Should', u'she', u'do', u'a', u'small', u'gif', u'spam', u'?'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'Keep', u'Christ', u'in', u'Christmas', u'?', u'Can', u'we', u'put', u'him', u'back', u'in', u'Christianity', u'first', u'?'] \n",
      "\n",
      "0 [u'Happy', u'Black', u'Friday', u'!', u'Had', u'enough', u'shopping', u'and', u'turkey', u'to', u'hold', u'you', u'over', u'for', u'a', u'while', u'?', u'Do', u\"n't\", u'worry', u',', u'\\u2026', u'URL8'] \n",
      "\n",
      "0 [u'How', u'can', u'you', u'post', u'a', u'video', u'with', u'music', u'?', u'I', u'keep', u'getting', u'pied', u'\\U0001f602\\U0001f602'] \n",
      "\n",
      "0 [u'TOUSER', u'why', u'must', u'you', u'be', u'ridiculously', u'photogenic', u'?'] \n",
      "\n",
      "0 [u'TOUSER', u'Sorry', u'to', u'hear', u'about', u'the', u'trouble', u'!', u'Do', u'any', u'error', u'messages', u'pop', u'up', u'when', u'the', u'app', u'crashes', u'?', u'For', u'now', u',', u'try', u':', u'URL1', u'.'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'Trump', u'is', u'passing', u'out', u'potato', u'chips', u'and', u'turkey', u'sandwiches', u'.', u'This', u'is', u'the', u'best', u'they', u'can', u'do', u'for', u'Thanksgiving', u'?', u'URL'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'THIS', u'IS', u'HOW', u'A', u'DATE', u'SHOULD', u'BE', u'PLANNED', u'OUT', u'instead', u'of', u':', u'\\u201c', u'what', u'do', u'you', u'wan', u'na', u'do', u'?', u'\\u201d', u'.', u'Ask', u'your', u'girl', u'on', u'a', u'date', u'with', u'plans', u'already', u'm\\u2026'] \n",
      "\n",
      "0 [u'What', u'are', u'the', u'best', u'herbs', u'for', u'stinky', u\"'pits\", u'?'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'Is', u'that', u'Martin', u'McGuinness', u'doing', u'his', u'assistant', u'on', u'the', u'Sevco', u'bench', u'?', u'?', u'?', u'\\U0001f648', u'URL8'] \n",
      "\n",
      "0 [u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'You', u'are', u'a', u'bot', u',', u'right', u'?', u'At', u'least', u'you', u'sound', u'like', u'one', u'.', u'Improve', u'the', u'technology', u'!'] \n",
      "\n",
      "0 [u'TOUSER', u'Dang', u'He', u'Hurt', u'You', u'That', u'Bad', u'?', u'\\U0001f60f'] \n",
      "\n",
      "0 [u'TOUSER', u'Why', u'do', u'people', u'have', u'such', u'a', u'Big', u'problem', u'with', u'supporting', u'each', u'other', u'?', u'How', u'is', u'a', u'rt', u'going', u'to', u'kill', u'you', u'?', u'Smh'] \n",
      "\n",
      "0 [u'Here', u'we', u'have', u'the', u'unofficial', u',', u'official', u'defender', u'of', u'the', u'little', u'Hitlers', u'defending', u'their', u'right', u'to', u'silence', u'hundreds', u'of', u'thousands', u'indiscriminately', u'.', u'Do', u'you', u'buy', u'this', u'\\u201c', u'reasoning', u'\\u201d', u'?', u'No', u',', u'I', u'didn', u'\\u2019', u't', u'think', u'any', u'thinking', u'same', u'person', u'would', u'.', u'U', u'picked', u'the', u'wrong', u'woman', u'to', u'try', u'your', u'brand', u'of', u'reasoning', u'on', u'URL'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'Thanksgiving', u'?', u'Well', u'mind', u'stuffing', u'this', u'Imp', u'up', u'?', u'~', u'URL'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'knock', u'knock', u'who', u\"'s\", u'there', u'?', u'not', u'pinof', u'9', u'apparently'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'wait', u'why', u'do', u'people', u'think', u'the', u'UK', u'dates', u'are', u'going', u'to', u'be', u'the', u'only', u'Europe', u'dates', u'?', u'?', u'she', u'would', u\"n't\", u'only', u'do', u'one', u'part', u'of', u'Europe', u'th\\u2026'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'171124', u'nct127', u'ig', u'live', u'#', u'JAEHYUN', u'JH', u':', u'``', u'We', u\"'re\", u'practicing', u'together', u'now..', u'with', u'all', u'127', u'members..', u'what', u'is', u'it', u'for~', u'?', u'it', u\"'s\", u'somethi\\u2026'] \n",
      "\n",
      "0 [u'TOUSER', u'After', u'Miam', u'loses', u'to', u'TOUSER', u'and', u'TOUSER', u'beats', u'TOUSER', u'how', u'about', u'a', u'rematch', u'with', u'TOUSER', u'?'] \n",
      "\n",
      "0 [u'TOUSER', u'Am', u'I', u'ready', u'for', u'this', u'type', u'of', u'greatness', u'?', u'URL'] \n",
      "\n",
      "0 [u'MONKEY', u'BUSINESS', u'|', u'remember', u'this', u'guy', u'?', u'I', u'\\u2019', u've', u'been', u'very', u'busy', u'working', u'on', u'sole', u'cool', u'projects', u'but', u'I', u'took', u'a', u'break', u'to', u'do', u'this', u'little', u'guy', u'.', u'Hopefully', u'next', u'year', u'it', u'will', u'be', u'finally', u'a', u'book', u'.', u'.', u'.', u'.', u'.', u'#', u'luisgadea', u'#', u'gadea', u'#', u'monkey', u'#', u'animals', u'#', u'characterdesign', u'#', u'illustration', u'#', u'traditionalart', u'#', u'artistso\\u2026', u'URL'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'``', u'DID', u'YOU', u'THANK', u'THE', u'DOCTOR', u'FOR', u'BRINGING', u'YOU', u'INTO', u'THIS', u'WORLD', u'?', u\"''\", u'Lavar', u'Ball', u'is', u'PURE', u'comedy', u'!', u'\\U0001f602\\U0001f480', u'URL'] \n",
      "\n",
      "0 [u'TOUSER', u'TOUSER', u'Um', u'...', u'Let', u\"'s\", u'go', u'Bulls', u'?'] \n",
      "\n",
      "0 [u'I', u'finally', u'learned', u'that', u'if', u'I', u'read', u'enough', u'I', u'wo', u\"n't\", u'have', u'to', u'worry', u'about', u'accidentally', u'plagiarizing', u'anymore', u'.', u'Plausible', u'deniability', u'and', u'all', u'.', u'Have', u'any', u'other', u'writers', u'worried', u'about', u'this', u'?', u'URLD'] \n",
      "\n",
      "0 [u'TOUSER', u'I', u'\\u2019', u'm', u'wrong', u'or', u'right', u'?'] \n",
      "\n",
      "0 [u'I', u'told', u'them', u'not', u'to', u'screw', u'it', u'up', u'...', u'What', u'do', u'they', u'do', u'?', u'...', u'Screw', u'it', u'up', u'smh', u'.', u'URL'] \n",
      "\n",
      "0 [u'TOUSER', u'And', u'lose', u'money', u'as', u'usual', u'?'] \n",
      "\n",
      "0 [u'Have', u'you', u'forgotten', u'the', u'3', u'second', u'rule', u'?', u'This', u'video', u'from', u'TOUSER', u'shows', u'you', u'how', u'to', u'better', u'avoid', u'accidents', u'.', u'URL'] \n",
      "\n",
      "0 [u'#', u'ThisDayInHistory', u'1947', u':', u'House', u'of', u'Representatives', u'cites', u'#', u'Hollywood', u'Ten', u'for', u'contempt', u'of', u'#', u'Congress', u'.', u'#', u'Commies', u'or', u'Not', u'Commies', u'?', u'Read', u'more', u'here', u'URL', u'#', u'ushistory', u'#', u'ColdWar', u'URL10'] \n",
      "\n",
      "0 [u'#', u'women', u'are', u'under', u'the', u'most', u'brutal', u'suppression', u'in', u'#', u'Iran', u',', u'I', u\"'m\", u'surprised', u'that', u'you', u\"'re\", u'so', u'eager', u'2go', u'there', u'!', u'Is', u'this', u'mission', u'on', u'behalf', u'of', u'TOUSER', u'?', u'Or', u'is', u'your', u'delegation', u'working', u'on', u'trade', u'agreements', u'W/', u'Iran', u'?', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'URL'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'TOUSER', u'TOUSER', u'So', u'their', u'illegal', u'and', u'want', u'to', u'bring', u'attention', u'to', u'themselves', u'?', u'\\U0001f914'] \n",
      "\n",
      "0 [u'Wait', u',', u'what', u'?', u'Someone', u'explain', u'what', u'this', u'means', u'.', u'URL'] \n",
      "\n",
      "0 [u'TOUSER', u'what', u'is', u'this', u'?', u'URL'] \n",
      "\n",
      "0 [u'Are', u'there', u'any', u'wireless', u'controllers', u'out', u'there', u'for', u'the', u'SNES', u'Classic', u'available', u'for', u'Canada', u'?', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'TOUSER', u'#', u'PleaseHelp', u'#', u'GameON'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'What', u'do', u'you', u'think', u'of', u'Christ', u'?', u'May', u'we', u'reflect', u'on', u't/greatest', u'blessing', u'of', u'all', u',', u't/true', u'nature', u'of', u'Christ', u'our', u'Lord', u'!', u'Jesus', u',', u'no', u'o\\u2026'] \n",
      "\n",
      "0 [u'TOUSER', u'Is', u'there', u'another', u'view', u'of', u'this', u'?', u'It', u'does', u'look', u'like', u'it', u'could', u'get', u'swiped', u'at', u'as', u'he', u'\\u2019', u's', u'going', u'up', u'.'] \n",
      "\n",
      "0 [u'Found', u'a', u'Transponder', u'Snail', u'!', u'Oars', u'comes', u'back', u'as', u'a', u'henchman', u'of', u'Moria', u'?', u'!', u'URL', u'#', u'TreCru', u'URL'] \n",
      "\n",
      "0 [u'TOUSER', u'\\U0001f928', u'is', u'it', u'bc', u'you', u'want', u'it', u'yourself', u'?', u'or', u'...', u'another', u'reason', u'?'] \n",
      "\n",
      "0 [u'TOUSER', u'im', u'early', u'are', u'you', u'proud', u'?', u'URL'] \n",
      "\n",
      "0 [u'Anybody', u'wan', u'na', u'chill', u'in', u'a', u'party', u'or', u'play', u'pro-am', u'?'] \n",
      "\n",
      "0 [u'May', u'be', u'a', u'coincidence', u',', u'but', u'did', u\"n't\", u'Egypt', u'JUST', u'open', u'its', u'border', u'with', u'Gaza', u'strip', u'recently', u'after', u'a', u'long', u'shutdown', u'?', u'Hmm', u'.', u'``', u'Militants', u'Kill', u'235', u'in', u'Attack', u'on', u'Sufi', u'Mosque', u'in', u'Egypt', u\"''\", u'NYT', u'URL'] \n",
      "\n",
      "0 [u'TOUSER', u'TOUSER', u'Will', u'Volume', u'4', u'ever', u'be', u'reprinted', u'?'] \n",
      "\n",
      "0 [u'What', u'needs', u'to', u'happen', u'to', u'get', u'Central', u'Florida', u'into', u'the', u'playoff', u'?', u'Because', u'that', u\"'s\", u'what', u'I', u\"'m\", u'rooting', u'for', u'.'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'BAEK', u'new', u'``', u'fearless', u\"''\", u'tattoo', u'?', u'?', u'\\U0001f440', u'#', u'EXO', u'#', u'TheElyXiOninSeoulday1', u'TOUSER', u'URL2C'] \n",
      "\n",
      "0 [u'TOUSER', u'Lmaoooooooo', u'whet', u'?', u'!', u'You', u'so', u'out', u'of', u'order', u'\\U0001f62d\\U0001f62d'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'Did', u'u', u'know', u'MI', u'has', u'the', u'U.S.', u'1st', u'state-focused', u'#', u'connectedtech', u'trade', u'assn', u'?', u'Let', u'us', u'be', u'ur', u'resource', u'!', u'URL', u'#', u'mobili\\u2026'] \n",
      "\n",
      "0 [u'TOUSER', u'TOUSER', u'Next', u'time', u'you', u\"'re\", u'coming', u'TOUSER', u'right', u'?'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'#', u'PINOF9', u'Do', u'you', u'want', u'Pinof', u'9', u'to', u'be', u'released', u'now', u'?', u'-vote', u'and', u'rt', u'pls-'] \n",
      "\n",
      "0 [u'TOUSER', u'This', u'is', u'the', u'only', u'acceptable', u'outcome', u'?', u'?', u'?', u'Instrument', u'?', u'?', u'?', u'His', u'hot', u'bod'] \n",
      "\n",
      "0 [u'Some', u'people', u'are', u'so', u'hard', u'to', u'understand', u'.', u'?'] \n",
      "\n",
      "0 [u'RT', u'TOUSER', u':', u'Catfishing', u'ah', u'now', u'could', u'ye', u'be', u'bothered', u'?'] \n",
      "\n",
      "0 [u'//', u'I', u'LOVE', u'how', u'whenever', u'my', u'Snart', u'says', u'something', u'like', u'\\u2018', u'Anyone', u'wants', u'to', u'play', u'?', u'\\u2019', u'and', u'another', u'Snart', u'comes', u'to', u'respond', u'lmao', u'it', u'\\u2019', u's', u'like', u'\\u2018', u'Ah', u',', u'I', u'got', u'you', u',', u'my', u'lonely', u'other', u'self.', u'\\u2019', u'It', u'\\u2019', u's', u'cute', u'.', u'Lol', u'.'] \n",
      "\n",
      "0 [u'TOUSER', u'TOUSER', u'When', u'can', u'we', u'expect', u'an', u'update', u'?'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = tokenize_tweets(tweets)\n",
    "visualize(tweet_tokens, y, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_negation(w):\n",
    "    global NEGATIONS\n",
    "    if w.lower() in NEGATIONS:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_modal(w):\n",
    "    global MODAL_VERBS\n",
    "    if w.lower() in MODAL_VERBS:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def handcrafted_features(tok_tweets):\n",
    "    \"\"\"\n",
    "    Returns samples,2 matrix\n",
    "    1st col: MODAL present in tweet then 1\n",
    "    2nd col: NEGATION present in tweet then 1\n",
    "    3rd col: position of 1st question mark / total num words\n",
    "    \"\"\"\n",
    "    f = np.zeros((len(tok_tweets), 3))\n",
    "    for i,toks in enumerate(tok_tweets):\n",
    "        features_set = 0\n",
    "        for tok in toks:\n",
    "            if is_modal(tok):\n",
    "                f[i][0] = 1\n",
    "                features_set += 1\n",
    "            if is_negation(tok):\n",
    "                f[i][1] = 1\n",
    "                features_set += 1\n",
    "            if features_set == 2:\n",
    "                break\n",
    "        try:\n",
    "            first_pos = toks.index('?') + 1.0\n",
    "            f[i][2] = first_pos / len(toks)\n",
    "        except ValueError:\n",
    "            f[i][2] = 1\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_feats = handcrafted_features(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7483, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.875     ],\n",
       "       [ 0.        ,  1.        ,  0.45454545],\n",
       "       [ 1.        ,  0.        ,  0.63333333],\n",
       "       [ 0.        ,  0.        ,  0.375     ],\n",
       "       [ 0.        ,  1.        ,  0.4375    ],\n",
       "       [ 0.        ,  0.        ,  0.95      ],\n",
       "       [ 0.        ,  0.        ,  0.42857143],\n",
       "       [ 0.        ,  1.        ,  0.67567568],\n",
       "       [ 1.        ,  0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ,  0.67741935]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print hand_feats.shape\n",
    "hand_feats[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets2emb(tweet_tokens, emb):  \n",
    "    \"\"\"\n",
    "    Convert list of tokenized tweets to sum of embedding\n",
    "    tweet_tokens: list of list of words\n",
    "    emb: embedding table\n",
    "    \n",
    "    returns (num_samples x emb_dim)\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    for toks in tweet_tokens:\n",
    "        x_i = np.zeros((1,100))\n",
    "        for tok in toks:\n",
    "            # handle captilized words\n",
    "            if tok.upper() != tok:\n",
    "                tok = tok.lower()\n",
    "\n",
    "            if tok in emb:\n",
    "                x_i += emb[tok]\n",
    "\n",
    "            else:\n",
    "                if tok[0] == '#':\n",
    "                    x_i += emb['HASHTAG']\n",
    "\n",
    "                elif tok == 'TOUSER':\n",
    "                    x_i += emb['@touser']\n",
    "\n",
    "                elif tok == 'TOUSER':\n",
    "                    x_i += emb['@touser']\n",
    "\n",
    "                elif contains_digit(tok):\n",
    "                    x_i += emb['NUMBER']\n",
    "\n",
    "                elif not contains_alpha(tok):\n",
    "                    x_i += emb['CHARACTER']\n",
    "\n",
    "                else:\n",
    "                    x_i += emb['UNKNOWN']     \n",
    "        x.append(x_i)\n",
    "    x = np.concatenate(x, axis=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7483, 100)\n",
      "(7483,)\n"
     ]
    }
   ],
   "source": [
    "x = tweets2emb(tweet_tokens, emb)\n",
    "print x.shape\n",
    "print y_integer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate embeddings and handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7483, 103)\n"
     ]
    }
   ],
   "source": [
    "x = np.concatenate([x, hand_feats], axis=1)\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y_integer, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(class_weight='balanced') # default is rbf kernel\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\tNot RQ  \tRQ  \t        Overall\n",
      "precis\t0.756026296567\t0.740236148955\t0.748131222761\n",
      "recall\t0.783497350492\t0.709312445605\t0.746404898048\n",
      "f1\t0.769516728625\t0.724444444444\t0.746980586534\n"
     ]
    }
   ],
   "source": [
    "# clf.score(x_test, y_test)\n",
    "pred = clf.predict(x_test)\n",
    "precision, recall, f1, _ = score(y_test, pred)\n",
    "precisiont, recallt, f1t, _ = score(y_test, pred, average='macro')\n",
    "print 'Score\\tNot RQ  \\tRQ  \\t        Overall'\n",
    "print 'precis\\t{}\\t{}\\t{}'.format(precision[0], precision[1], precisiont)\n",
    "print 'recall\\t{}\\t{}\\t{}'.format(recall[0], recall[1], recallt)\n",
    "print 'f1\\t{}\\t{}\\t{}'.format(f1[0], f1[1], f1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try linear svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf = svm.SVC(kernel='linear', class_weight='balanced') # default is rbf kernel\n",
    "lin_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\tNot RQ  \tRQ  \t        Overall\n",
      "precis\t0.734678624813\t0.701413427562\t0.718046026187\n",
      "recall\t0.7441332324\t0.691035683203\t0.717584457801\n",
      "f1\t0.739375705152\t0.696185883384\t0.717780794268\n"
     ]
    }
   ],
   "source": [
    "pred_lin = lin_clf.predict(x_test)\n",
    "precision, recall, f1, _ = score(y_test, pred_lin)\n",
    "precisiont, recallt, f1t, _ = score(y_test, pred_lin, average='macro')\n",
    "print 'Score\\tNot RQ  \\tRQ  \\t        Overall'\n",
    "print 'precis\\t{}\\t{}\\t{}'.format(precision[0], precision[1], precisiont)\n",
    "print 'recall\\t{}\\t{}\\t{}'.format(recall[0], recall[1], recallt)\n",
    "print 'f1\\t{}\\t{}\\t{}'.format(f1[0], f1[1], f1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [WIP] LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-317c0a85bdff>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-317c0a85bdff>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    def forward(self, sentence):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class RQClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RQClassifier, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RQClassifier(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# # See what the scores are before training\n",
    "# # Note that element i,j of the output is the score for tag j for word i.\n",
    "# inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# print(tag_scores)\n",
    "\n",
    "# for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "#     for sentence, tags in training_data:\n",
    "#         # Step 1. Remember that Pytorch accumulates gradients.\n",
    "#         # We need to clear them out before each instance\n",
    "#         model.zero_grad()\n",
    "\n",
    "#         # Also, we need to clear out the hidden state of the LSTM,\n",
    "#         # detaching it from its history on the last instance.\n",
    "#         model.hidden = model.init_hidden()\n",
    "\n",
    "#         # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "#         # Variables of word indices.\n",
    "#         sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "#         targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "#         # Step 3. Run our forward pass.\n",
    "#         tag_scores = model(sentence_in)\n",
    "\n",
    "#         # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "#         #  calling optimizer.step()\n",
    "#         loss = loss_function(tag_scores, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # See what the scores are after training\n",
    "# inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "# #  for word i. The predicted tag is the maximum scoring tag.\n",
    "# # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# # since 0 is index of the maximum value of row 1,\n",
    "# # 1 is the index of maximum value of row 2, etc.\n",
    "# # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "# print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data, clean it, evaluate on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RQ: 26\n",
      "Not RQ: 30\n",
      "Total samples: 56\n",
      "1 Real good friend, A+ to you ? \n",
      "\n",
      "1 Did you know there are people in the world that actually WANT Hillary Clinton for president?! #depressingfact \n",
      "\n",
      "1 Isn't it cool how the state spends our tax money on building new jails instead of schools? #facepalm \n",
      "\n",
      "1 Oh weird, the NFL referees screwed up? That's NEVER happened before. \n",
      "\n",
      "1 I sprayed perfume in my eye! I am now a genius!! ? \n",
      "\n",
      "1 Glad people keep their promises these days ? \n",
      "\n",
      "0 Alabama is awesome. \n",
      "\n",
      "0 An afternoon of some shopping!! Luca had a blast. #bestfriends http://t.co/sG8DPHiizq \n",
      "\n",
      "0 ToUser never too many Italians at one party.. \n",
      "\n",
      "0 Because being on #tmt is more important than getting an education.. Good shit nigga. Glad your priorities are on point. \n",
      "\n",
      "0 As much as I love some world history I really wanna read about 95 theses of a religion I'm not apart off \n",
      "\n",
      "0 So excited to get the bus back to aberdeen tonight. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rq_file2 = 'rq_test.txt'\n",
    "not_rq_file2 = 'notrq_test.txt'\n",
    "\n",
    "tweets_raw2,y2 = load_data(rq_file2, not_rq_file2)\n",
    "y_integer2 = [np.argmax(y_i) for y_i in y2] # store integer labels instead of one-hot vec\n",
    "y_integer2 = np.array(y_integer2)\n",
    "\n",
    "tweets2 = [remove_taboo(t) for t in tweets_raw2]\n",
    "visualize(tweets2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Real good friend, A+ to you ? \n",
      "\n",
      "1 Did you know there are people in the world that actually WANT Hillary Clinton for president?! #depressingfact \n",
      "\n",
      "1 Isn't it cool how the state spends our tax money on building new jails instead of schools? #facepalm \n",
      "\n",
      "1 Oh weird, the NFL referees screwed up? That's NEVER happened before. \n",
      "\n",
      "1 I sprayed perfume in my eye! I am now a genius!! ? \n",
      "\n",
      "1 Glad people keep their promises these days ? \n",
      "\n",
      "0 Alabama is awesome. \n",
      "\n",
      "0 An afternoon of some shopping!! Luca had a blast. #bestfriends URL \n",
      "\n",
      "0 ToUser never too many Italians at one party.. \n",
      "\n",
      "0 Because being on #tmt is more important than getting an education.. Good shit nigga. Glad your priorities are on point. \n",
      "\n",
      "0 As much as I love some world history I really wanna read about 95 theses of a religion I'm not apart off \n",
      "\n",
      "0 So excited to get the bus back to aberdeen tonight. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets2 = replace_special(tweets2)\n",
    "visualize(tweets2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [u'Real', u'good', u'friend', u',', u'A+', u'to', u'you', u'?'] \n",
      "\n",
      "1 [u'Did', u'you', u'know', u'there', u'are', u'people', u'in', u'the', u'world', u'that', u'actually', u'WANT', u'Hillary', u'Clinton', u'for', u'president', u'?', u'!', u'#', u'depressingfact'] \n",
      "\n",
      "1 [u'Is', u\"n't\", u'it', u'cool', u'how', u'the', u'state', u'spends', u'our', u'tax', u'money', u'on', u'building', u'new', u'jails', u'instead', u'of', u'schools', u'?', u'#', u'facepalm'] \n",
      "\n",
      "1 [u'Oh', u'weird', u',', u'the', u'NFL', u'referees', u'screwed', u'up', u'?', u'That', u\"'s\", u'NEVER', u'happened', u'before', u'.'] \n",
      "\n",
      "1 [u'I', u'sprayed', u'perfume', u'in', u'my', u'eye', u'!', u'I', u'am', u'now', u'a', u'genius', u'!', u'!', u'?'] \n",
      "\n",
      "1 [u'Glad', u'people', u'keep', u'their', u'promises', u'these', u'days', u'?'] \n",
      "\n",
      "0 [u'Alabama', u'is', u'awesome', u'.'] \n",
      "\n",
      "0 [u'An', u'afternoon', u'of', u'some', u'shopping', u'!', u'!', u'Luca', u'had', u'a', u'blast', u'.', u'#', u'bestfriends', u'URL'] \n",
      "\n",
      "0 [u'ToUser', u'never', u'too', u'many', u'Italians', u'at', u'one', u'party..'] \n",
      "\n",
      "0 [u'Because', u'being', u'on', u'#', u'tmt', u'is', u'more', u'important', u'than', u'getting', u'an', u'education..', u'Good', u'shit', u'nigga', u'.', u'Glad', u'your', u'priorities', u'are', u'on', u'point', u'.'] \n",
      "\n",
      "0 [u'As', u'much', u'as', u'I', u'love', u'some', u'world', u'history', u'I', u'really', u'wan', u'na', u'read', u'about', u'95', u'theses', u'of', u'a', u'religion', u'I', u\"'m\", u'not', u'apart', u'off'] \n",
      "\n",
      "0 [u'So', u'excited', u'to', u'get', u'the', u'bus', u'back', u'to', u'aberdeen', u'tonight', u'.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets2 = tokenize_tweets(tweets2)\n",
    "visualize(tweets2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 3)\n",
      "(56, 100)\n"
     ]
    }
   ],
   "source": [
    "hand_feats2 = handcrafted_features(tweets2)\n",
    "x_test2 = tweets2emb(tweets2,emb)\n",
    "print hand_feats2.shape\n",
    "print x_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 103)\n"
     ]
    }
   ],
   "source": [
    "# concat embedding and handcrafted features\n",
    "x_test2 = np.concatenate([x_test2, hand_feats2], axis=1)\n",
    "print x_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\tNot RQ  \tRQ  \t        Overall\n",
      "precis\t0.6\t0.571428571429\t0.585714285714\n",
      "recall\t0.7\t0.461538461538\t0.580769230769\n",
      "f1\t0.646153846154\t0.510638297872\t0.578396072013\n"
     ]
    }
   ],
   "source": [
    "pred2 = clf.predict(x_test2)\n",
    "precision, recall, f1, support = score(y_integer2, pred2)\n",
    "precisiont, recallt, f1t, support = score(y_integer2, pred2, average='macro')\n",
    "print 'Score\\tNot RQ  \\tRQ  \\t        Overall'\n",
    "print 'precis\\t{}\\t{}\\t{}'.format(precision[0], precision[1], precisiont)\n",
    "print 'recall\\t{}\\t{}\\t{}'.format(recall[0], recall[1], recallt)\n",
    "print 'f1\\t{}\\t{}\\t{}'.format(f1[0], f1[1], f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Pred Tweet\n",
      "1 0 Real good friend, A+ to you ?\n",
      "1 1 You know what? Josh Freeman looks really good on this drive.\n",
      "1 1 After the game last night, seems to me like Fernando Torres is BACK!!!! ?? ?? #notreally #CFC\n",
      "1 0 Mr shey why are you so cool????\n",
      "1 0 Before the weekend gets started im gonna get started in this homework ?\n",
      "1 1 Did you know there are people in the world that actually WANT Hillary Clinton for president?! #depressingfact\n",
      "1 1 Did Holliday just get a homerun? I couldn't tell..\n",
      "1 1 Is Thanksgiving today? I would have never known if it weren't for social media..... Close call.\n",
      "1 1 Adios Wendy's see you tomorrow same time ? #work #don'twannago #justgotoff #bedtime http://t.co/8fz6o4Kbac\n",
      "1 0 I'm starting to enjoy cardio now ? ? http://t.co/KTWKvBEzFk\n",
      "1 1 Isn't it cool how the state spends our tax money on building new jails instead of schools? #facepalm\n",
      "1 1 Adios Wendy's see you tomorrow same time ? #work #don'twannago #justgotoff #bedtime http://t.co/8fz6o4Kbac\n",
      "1 0 Glad people keep their promises these days ?\n",
      "1 0 Could you be just a little louder, please? My baby isn't trying to sleep.\n",
      "1 0 ToUser Oo yeah cause you're soooo fat aren't you?\n",
      "1 1 Oh weird, the NFL referees screwed up? That's NEVER happened before.\n",
      "1 0 I have 60MB free. I wonder if I can install iOS 7.0? http://t.co/Jm9B0wXx6U\n",
      "1 0 Can you be annoying a little later? I'm still cleaning the gun...\n",
      "1 1 A shitty job, one sided conversations, and lonely nights... That's my life right now... Jealous?\n",
      "1 0 I'm starting to enjoy cardio now ? ? http://t.co/KTWKvBEzFk\n",
      "1 0 I sprayed perfume in my eye! I am now a genius!! ?\n",
      "1 0 fast internet connection, eh? @User\n",
      "1 1 Is there a Red Sox game going on?\n",
      "1 1 Always love after sending a long message simply getting okay back!!?? #okay?\n",
      "1 0 Don't you just looooove fighting with people? #blegh\n",
      "1 0 Glad people keep their promises these days ?\n",
      "0 0 Nothing like some good old 1950s polka music...\n",
      "0 0 ToUser Other than the 38 unanswered points by ucla I thought we did great!\n",
      "0 1 After 7 hours of work, there's nothing I look forward to more than 2 more hours of work. #ineedanap\n",
      "0 1 Ahh I'm so glad I'm at work already makes me think how blessed I am ha #cannotbeassed\n",
      "0 0 Alabama is awesome.\n",
      "0 1 All day with kinder kids and now a lesson plan....WOOOH I LOVE THE TEACHER LIFE\n",
      "0 0 all I've eaten so far today is chocolate. that explains how amazing my day was\n",
      "0 0 All nighter at the hospital ......again #yes mylife\n",
      "0 0 All your friends hate me, that makes me feel really good.\n",
      "0 0 An afternoon of some shopping!! Luca had a blast. #bestfriends http://t.co/sG8DPHiizq\n",
      "0 0 ToUser See me out. Add me to Facebook. Poke me. Like all my pics. Ya that'll make me talk to you. #GoAway\n",
      "0 1 @User I think there is a new Hunger Games movie coming out soon. Haven't really seen it advertised on TV that much though.\n",
      "0 0 ToUser Please continue to tweet every single thought that runs through your head. We're all dying to know.\n",
      "0 0 ToUser wow looks just like me\n",
      "0 0 ToUser never too many Italians at one party..\n",
      "0 1 Breaking Bad marathon from seasons 1 to 2. Perfect for homework productivity. Huhu. #duh #niceshowthough\n",
      "0 0 Nothing tops off a 10 hour day consisting of two midterms better, then running to catch the train.\n",
      "0 1 Again damn you society for trying to oversimplify complicated feelings, ours been greatly appreciated these past years\n",
      "0 0 Basically iOS7 copied certain aspects of Android. Wow, how innovative.\n",
      "0 1 Because being on #tmt is more important than getting an education.. Good shit nigga. Glad your priorities are on point.\n",
      "0 0 Did great on my college picks today\n",
      "0 1 I love it when people who are completely full of shit 100% of the time can get whatever they want\n",
      "0 0 maths first tomorrow, great way to start the day\n",
      "0 0 Great i inherited all of my mother's \"GOOD\" genes\n",
      "0 0 As much as I love some world history I really wanna read about 95 theses of a religion I'm not apart off\n",
      "0 1 I have to count trees on campus all the way down to the District Office during my TA Hour. Its sooooo fun! (: It\n",
      "0 0 Really looking forward to my bike ride home from work. #cycling #raining\n",
      "0 0 Wow, I hope I grow up to be exactly like you #never\n",
      "0 0 Always so pleasant talking to you... #Shush\n",
      "0 0 So excited to get the bus back to aberdeen tonight.\n"
     ]
    }
   ],
   "source": [
    "print 'Gold Pred Tweet'\n",
    "for i in range(pred2.shape[0]):\n",
    "    print '{} {} {}'.format(y_integer2[i], pred2[i], tweets_raw2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
